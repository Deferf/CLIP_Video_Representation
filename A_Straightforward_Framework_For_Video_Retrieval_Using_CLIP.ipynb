{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A Straightforward Framework For Video Retrieval Using CLIP",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE_aQ8yOVtkh"
      },
      "source": [
        "# Tests\n",
        "\n",
        "This notebook is concerned with results from the paper A Straightforward Framework For Video Retrieval Using CLIP [1].\n",
        "\n",
        "Recall metrics are calculated using a library provided by the author. Results are stored in `test.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLBWqlYjXZsN"
      },
      "source": [
        "# Modules\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ9BRqOFXDvB"
      },
      "source": [
        "# MSR-VTT\n",
        "Split:\n",
        "* Validation: 1000 videos from val set for single frame, average frame and $k$-means experiments.\n",
        "  * 30th frame as video representation.\n",
        "  * $k$-means for video representation.\n",
        "* Full Test: 2990 videos, 20 captions each.\n",
        "  * Average frame video representation.\n",
        "* JSFusion Test: 1000 sampled video-text pairs.\n",
        "  * Average frame video representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71gLeYCL2xOz",
        "outputId": "f2a04e31-6649-4b9b-b627-ff43263eba63"
      },
      "source": [
        "!gdown --id 1Gp3_I_OvcKwjOQmn334-T4wfwQk29TCp -O \"MSRVTT_test_dict_CLIP_visual.pt\"\n",
        "!gdown --id 1-3tpfZzo1_D18WdrioQzc-iogEl-KSnA -O \"MSRVTT_test_dict_CLIP_text.pt\"\n",
        "!gdown --id 1-7_zAogZjLLoaUvZa9i0OotvK81BDVDS -O \"MSRVTT_train_dict_Kmeans_centers_CLIP_video.pt\"\n",
        "!gdown --id 1-3PBg8qnLxe7AH008fl_WqfnPHi5Ror2 -O \"MSRVTT_val_1000_dict_sentence_0_CLIP_text.pt\"\n",
        "!gdown --id 1AiptvnIiObxGd_K9zDoOPvMjbJ1p2Rzr -O \"MSRVTT_val_1000_dict_CLIP_visual.pt\"\n",
        "!gdown --id 15mvFQxrWLNvBvFg4_9rr_Kqyzsy9dudj -O \"JS_test_dict_CLIP_text.pt\"\n",
        "!gdown --id 1DCPlt4zHJhatd3E2_9SW9ipeMKzy5Z0v -O \"MSVD_test_dict_CLIP_visual.pt\"\n",
        "!gdown --id 1ZkyeC1spejKXo8eAG3cCa2_RGEJg2MSV -O \"MSVD_test_dict_CLIP_text.pt\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Gp3_I_OvcKwjOQmn334-T4wfwQk29TCp\n",
            "To: /content/MSRVTT_test_dict_CLIP_visual.pt\n",
            "2.53GB [00:15, 162MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-3tpfZzo1_D18WdrioQzc-iogEl-KSnA\n",
            "To: /content/MSRVTT_test_dict_CLIP_text.pt\n",
            "123MB [00:00, 151MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7_zAogZjLLoaUvZa9i0OotvK81BDVDS\n",
            "To: /content/MSRVTT_train_dict_Kmeans_centers_CLIP_video.pt\n",
            "116MB [00:01, 99.0MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-3PBg8qnLxe7AH008fl_WqfnPHi5Ror2\n",
            "To: /content/MSRVTT_val_1000_dict_sentence_0_CLIP_text.pt\n",
            "41.3MB [00:00, 113MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AiptvnIiObxGd_K9zDoOPvMjbJ1p2Rzr\n",
            "To: /content/MSRVTT_val_1000_dict_CLIP_visual.pt\n",
            "819MB [00:08, 98.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15mvFQxrWLNvBvFg4_9rr_Kqyzsy9dudj\n",
            "To: /content/JS_test_dict_CLIP_text.pt\n",
            "2.17MB [00:00, 68.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DCPlt4zHJhatd3E2_9SW9ipeMKzy5Z0v\n",
            "To: /content/MSVD_test_dict_CLIP_visual.pt\n",
            "393MB [00:03, 130MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZkyeC1spejKXo8eAG3cCa2_RGEJg2MSV\n",
            "To: /content/MSVD_test_dict_CLIP_text.pt\n",
            "57.1MB [00:00, 137MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBpDYnJpaJSg",
        "outputId": "9d89b898-78af-4a9c-ac57-c0acf6d0be0e"
      },
      "source": [
        "!git clone https://github.com/Deferf/Experiments"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Experiments'...\n",
            "remote: Enumerating objects: 140, done.\u001b[K\n",
            "remote: Counting objects: 100% (140/140), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 140 (delta 84), reused 101 (delta 45), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (140/140), 22.34 KiB | 7.45 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71eLV3nYYKCN",
        "outputId": "cdb3aea4-1e23-428b-e60d-1e961459f7cf"
      },
      "source": [
        "%cd Experiments\n",
        "from metrics import rank_at_k_precomputed,stack_encoded_dict,generate_sim_tensor,tensor_video_to_text_sim,tensor_text_to_video_metrics,normalize_matrix\n",
        "%cd \"/content\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Experiments\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFyDpdIg74lp"
      },
      "source": [
        "MSR_path = \"/content/\"\n",
        "MSVD_path = \"/content/\"\n",
        "LSMDC_path = \"/content/\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0-5XBK07mO4"
      },
      "source": [
        "MSR_test_video_dict = torch.load(MSR_path + \"MSRVTT_test_dict_CLIP_visual.pt\")\n",
        "MSR_test_text_dict = torch.load(MSR_path + \"MSRVTT_test_dict_CLIP_text.pt\")\n",
        "MSR_cluster_centers_dict = torch.load(MSR_path + \"MSRVTT_train_dict_Kmeans_centers_CLIP_video.pt\", map_location = \"cpu\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twf747v5j7ci"
      },
      "source": [
        "## Preliminar Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suRyiLvFoxVH"
      },
      "source": [
        "# Text features from the 1000 validation videos\n",
        "val_1000_text_dict = torch.load(MSR_path + \"MSRVTT_val_1000_dict_sentence_0_CLIP_text.pt\", map_location = \"cpu\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_EGUJCHR8Nz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjqqdG98kBJO"
      },
      "source": [
        "### Single frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nslqY8L1koa0"
      },
      "source": [
        "val_order = list(MSR_cluster_centers_dict[1].keys())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Xv_bAOnnFD"
      },
      "source": [
        "val_1000_videos_dict = torch.load(MSR_path + \"MSRVTT_val_1000_dict_CLIP_visual.pt\", map_location = \"cpu\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVJYJ3csn2XQ"
      },
      "source": [
        "# Sample the 30th frame\n",
        "FRAME = 29"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ib7Dz3uwq52"
      },
      "source": [
        "val_1000_videos_30th = torch.stack([val_1000_videos_dict[key][FRAME] for key in val_order])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxnR5uLuxC4P"
      },
      "source": [
        "val_1000_text = torch.stack([val_1000_text_dict[key] for key in val_order])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzI8rW28rgUI"
      },
      "source": [
        "MSR_val_metrics_ttv_30th = rank_at_k_precomputed(val_1000_text @ val_1000_videos_30th.T)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dhfUmDjtlLv"
      },
      "source": [
        "MSR_val_metrics_ttv_30th[\"description\"] = [\"MSR-VTT Validation 1000 x (Val Set[\" + str(FRAME + 1) + \" frame], First sentence)\"]\n",
        "MSR_val_metrics_ttv_30th[\"task\"] = [\"Text-to-video\"]\n",
        "MSR_val_metrics_ttv_30th[\"dataset\"] = [\"MSR-VTT\"]\n",
        "MSR_val_metrics_ttv_30th = pd.DataFrame(MSR_val_metrics_ttv_30th)\n",
        "MSR_val_metrics_ttv_30th.to_csv(\"test.csv\", mode='a', header=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lArAemalSTnS"
      },
      "source": [
        "### K-Means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFLoqnoGSWYT"
      },
      "source": [
        "# Repeat evaluations for each centroid \n",
        "for center in MSR_cluster_centers_dict:\n",
        "  video_features = torch.stack([MSR_cluster_centers_dict[center][key] for key in val_order])\n",
        "  temp_center, aux_temp_center = stack_encoded_dict(MSR_cluster_centers_dict[center],val_order)\n",
        "  cluster_similarities = dict()\n",
        "  if center > 1:\n",
        "    similarity_map = (val_1000_text @ video_features.permute(1,2,0))\n",
        "    similarity_map, _ = torch.max(similarity_map, dim = 0)\n",
        "  else:\n",
        "    similarity_map = (val_1000_text @ video_features.T)\n",
        "  MSR_val_metrics_ttv = rank_at_k_precomputed(similarity_map)\n",
        "  MSR_val_metrics_ttv[\"description\"] = [\"MSR-VTT Validation 1000 x (Val Set[\" + str(center) + \" centroids], First sentence)\"]\n",
        "  MSR_val_metrics_ttv[\"task\"] = [\"Text-to-video\"]\n",
        "  MSR_val_metrics_ttv[\"dataset\"] = [\"MSR-VTT\"]\n",
        "  MSR_val_metrics_ttv = pd.DataFrame(MSR_val_metrics_ttv)\n",
        "  MSR_val_metrics_ttv.to_csv(\"test.csv\", mode='a', header=False)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDKSseWa7gbP"
      },
      "source": [
        "### Full Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzRRCMYXCgq"
      },
      "source": [
        "MSR_full_test_sim_tensor = generate_sim_tensor(MSR_test_text_dict, MSR_test_video_dict, MSR_test_text_dict.keys())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2bA3mXn_IzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50be9b8-1b37-4c88-8997-b02141deacd8"
      },
      "source": [
        "MSR_full_test_sim_tensor.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2990, 20, 2990])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b-UQO_JEPEH"
      },
      "source": [
        "Video to text retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7T8h2r0BNRq"
      },
      "source": [
        "MSR_video_text_sim = tensor_video_to_text_sim(MSR_full_test_sim_tensor)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip_ZZIkVC06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dad6412-69bd-4199-e91b-c14d4290ded8"
      },
      "source": [
        "MSR_video_text_sim.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2990, 2990])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koPBGOn2D1bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7174fd-8bdc-429d-c26f-a531494f6527"
      },
      "source": [
        "MSR_full_metrics_vtt = rank_at_k_precomputed(MSR_video_text_sim)\n",
        "\n",
        "MSR_full_metrics_vtt[\"description\"] = [\"MSR-VTT Test 2990 x (Test Set[mean frame], Corresponding sentences)\"]\n",
        "MSR_full_metrics_vtt[\"task\"] = [\"Video-to-text\"]\n",
        "MSR_full_metrics_vtt[\"dataset\"] = [\"MSR-VTT\"]\n",
        "MSR_full_metrics_vtt = pd.DataFrame(MSR_full_metrics_vtt)\n",
        "print(MSR_full_metrics_vtt)\n",
        "MSR_full_metrics_vtt.to_csv(\"test.csv\", mode='a', header=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         R@1        R@5  ...           task  dataset\n",
            "0  40.301003  69.732445  ...  Video-to-text  MSR-VTT\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldnfmorRER6y"
      },
      "source": [
        "Text to video retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6zGsVd5ETfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5257890c-aa8d-4dbd-9b0e-3aabb6b2ffff"
      },
      "source": [
        "MSR_full_metrics_ttv, MSR_diagonal = tensor_text_to_video_metrics(MSR_full_test_sim_tensor, return_ranks = True)\n",
        "MSR_full_metrics_ttv[\"description\"] = [\"MSR-VTT Test 2990 x (Test Set[mean frame], Corresponding sentences)\"]\n",
        "MSR_full_metrics_ttv[\"task\"] = [\"Text-to-video\"]\n",
        "MSR_full_metrics_ttv[\"dataset\"] = [\"MSR-VTT\"]\n",
        "MSR_full_metrics_ttv = pd.DataFrame(MSR_full_metrics_ttv)\n",
        "print(MSR_full_metrics_ttv)\n",
        "MSR_full_metrics_ttv.to_csv(\"test.csv\", mode='a', header=False)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         R@1        R@5  ...           task  dataset\n",
            "0  21.367893  41.138798  ...  Text-to-video  MSR-VTT\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5shKcSHZO8kN"
      },
      "source": [
        "### JS Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRJyUHwkSfgM"
      },
      "source": [
        "JS_text_encoded_dict = torch.load(MSR_path + \"JS_test_dict_CLIP_text.pt\", map_location=torch.device('cpu'))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBtfv8fiu0ky"
      },
      "source": [
        "JS_video_encoded_mean, _ = stack_encoded_dict(MSR_test_video_dict, JS_text_encoded_dict.keys(), lambda x : normalize_matrix(torch.mean(x, dim = 0, keepdim = True)))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVMXWEmBvIGE"
      },
      "source": [
        "JS_text_encoded = torch.stack([JS_text_encoded_dict[key] for key in JS_text_encoded_dict])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAXUaAXgTNdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381414d3-4c0b-4a3d-c2a7-3271c2c99169"
      },
      "source": [
        "JS_text_encoded.shape, JS_video_encoded_mean.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1000, 512]), torch.Size([1000, 512]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eLe_ktLTRui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf28497f-c87d-4c3e-f288-dfb273548c9e"
      },
      "source": [
        "JS_metrics_vtt = rank_at_k_precomputed(JS_video_encoded_mean @ JS_text_encoded.T)\n",
        "JS_metrics_vtt[\"description\"] = [\"MSR-VTT Test 1000 x (JS_Fusion_Split[mean frame], sampled sentence)\"]\n",
        "JS_metrics_vtt[\"task\"] = [\"Video-to-text\"]\n",
        "JS_metrics_vtt[\"dataset\"] = [\"MSR-VTT\"]\n",
        "JS_metrics_vtt = pd.DataFrame(JS_metrics_vtt)\n",
        "print(JS_metrics_vtt)\n",
        "JS_metrics_vtt.to_csv(\"test.csv\", mode='a', header=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         R@1        R@5  ...           task  dataset\n",
            "0  27.200001  51.700001  ...  Video-to-text  MSR-VTT\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMmlrD1bUVHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82948401-844b-4145-e0b3-9a342f337ac1"
      },
      "source": [
        "JS_metrics_ttv, JS_diagonal_ttv  =  rank_at_k_precomputed(JS_text_encoded @ JS_video_encoded_mean.T, diag = True)\n",
        "JS_metrics_ttv[\"description\"] = [\"MSR-VTT Test 1000 x (JS_Fusion_Split[mean frame], sampled sentence)\"]\n",
        "JS_metrics_ttv[\"task\"] = [\"Text-to-video\"]\n",
        "JS_metrics_ttv[\"dataset\"] = [\"MSR-VTT\"]\n",
        "JS_metrics_ttv = pd.DataFrame(JS_metrics_ttv)\n",
        "print(JS_metrics_ttv)\n",
        "JS_metrics_ttv.to_csv(\"test.csv\", mode='a', header=False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         R@1        R@5  ...           task  dataset\n",
            "0  31.200001  53.700001  ...  Text-to-video  MSR-VTT\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Wo7KK1YUyL"
      },
      "source": [
        "# MSVD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRRr5fqiOFKR"
      },
      "source": [
        "multiple_video_dict = torch.load(MSVD_path + \"MSVD_test_dict_CLIP_visual.pt\", map_location=torch.device('cpu'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5LgUrmdkygC"
      },
      "source": [
        "test_text_dict = torch.load(MSVD_path + \"MSVD_test_dict_CLIP_text.pt\", map_location=torch.device('cpu'))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDhiyiy9uCZx"
      },
      "source": [
        "MSVD_test_sim_tensor = generate_sim_tensor(test_text_dict, multiple_video_dict, test_text_dict.keys())"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRd9qqIluCb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57d7dde-eec0-4ebb-e3c3-e02850cfbcc6"
      },
      "source": [
        "MSVD_test_sim_tensor.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([670, 81, 670])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWcpWoE4uRQK"
      },
      "source": [
        "Video to text retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYUMNKN6uRQZ"
      },
      "source": [
        "MSVD_video_text_sim = tensor_video_to_text_sim(MSVD_test_sim_tensor)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b2VW1JCuRQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd6dfe9-770e-45a0-b39f-95a3edaaed27"
      },
      "source": [
        "MSVD_video_text_sim.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([670, 670])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8URLvn7suRQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4bcf1a-5389-4250-b47b-a44f5e2670c8"
      },
      "source": [
        "MSVD_full_metrics_vtt = rank_at_k_precomputed(MSVD_video_text_sim)\n",
        "\n",
        "MSVD_full_metrics_vtt[\"description\"] = [\"MSVD Test 670 x (Test Set[mean frame], Corresponding sentences)\"]\n",
        "MSVD_full_metrics_vtt[\"task\"] = [\"Video-to-text\"]\n",
        "MSVD_full_metrics_vtt[\"dataset\"] = [\"MSVD\"]\n",
        "MSVD_full_metrics_vtt = pd.DataFrame(MSVD_full_metrics_vtt)\n",
        "print(MSVD_full_metrics_vtt)\n",
        "MSVD_full_metrics_vtt.to_csv(\"test.csv\", mode='a', header=False)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         R@1        R@5  ...           task  dataset\n",
            "0  59.850746  85.223877  ...  Video-to-text     MSVD\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUj-t-lhuRQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa47937-3e6b-47f0-c293-5fed783cc7cb"
      },
      "source": [
        "MSVD_full_metrics_ttv = tensor_text_to_video_metrics(MSVD_test_sim_tensor)\n",
        "MSVD_full_metrics_ttv[\"description\"] = [\"MSVD Test 670 x (Test Set[mean frame], Corresponding sentences)\"]\n",
        "MSVD_full_metrics_ttv[\"task\"] = [\"Text-to-video\"]\n",
        "MSVD_full_metrics_ttv[\"dataset\"] = [\"MSVD\"]\n",
        "MSVD_full_metrics_ttv = pd.DataFrame(MSVD_full_metrics_ttv)\n",
        "print(MSVD_full_metrics_ttv)\n",
        "MSVD_full_metrics_ttv.to_csv(\"test.csv\", mode='a', header=False)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         R@1        R@5  ...           task  dataset\n",
            "0  37.009689  64.103302  ...  Text-to-video     MSVD\n",
            "\n",
            "[1 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXgba4ECzROp"
      },
      "source": [
        "\n",
        "# LSMDC\n",
        "\n",
        "LSMDC access is restricted, please obtain access to the dataset. We use extracted features from files listed in:\n",
        "\n",
        "```\n",
        "LSMDC16_challenge_1000_publictect.csv\n",
        "```\n",
        "\n",
        "Feel free to use the video processing code in the library.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hskErWtRzV3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "586bd5a3-22fc-4acb-e3ad-ae6146a43940"
      },
      "source": [
        "\"\"\"LSMDC_test_video_dict = torch.load(LSMDC_path + \"LSMDC_test_CLIP_visual_1_2.pt\",map_location=torch.device('cpu'))\n",
        "LSMDC_test_text_dict = torch.load(LSMDC_path + \"LSMDC_test_dict_CLIP_text.pt\",map_location=torch.device('cpu'))\n",
        "LSMDC_text_matrix, LSMDC_aux_text = stack_encoded_dict(LSMDC_test_text_dict, LSMDC_test_text_dict.keys())\n",
        "LSMDC_video_matrix, LSMDC_aux_video = stack_encoded_dict(LSMDC_test_video_dict, LSMDC_test_text_dict.keys(), lambda x : normalize_matrix(torch.mean(x, dim = 0, keepdim = True)))\n",
        "LSMDC_text_matrix.shape, LSMDC_video_matrix.shape\n",
        "LSMDC_metrics_vtt = rank_at_k_precomputed(LSMDC_video_matrix @ LSMDC_text_matrix.T)\n",
        "\n",
        "LSMDC_metrics_vtt[\"description\"] = [\"LSMDC Test 1000 x (Test Set[mean frame], Corresponding sentences)\"]\n",
        "LSMDC_metrics_vtt[\"task\"] = [\"Video-to-text\"]\n",
        "LSMDC_metrics_vtt[\"dataset\"] = [\"LSMDC\"]\n",
        "LSMDC_metrics_vtt = pd.DataFrame(LSMDC_metrics_vtt)\n",
        "print(LSMDC_metrics_vtt)\n",
        "LSMDC_metrics_vtt.to_csv(\"test.csv\", mode='a', header=False)\n",
        "LSMDC_metrics_ttv, LSMDC_diagonal_ttv = rank_at_k_precomputed(LSMDC_text_matrix @ LSMDC_video_matrix.T, diag = True)\n",
        "\n",
        "LSMDC_metrics_ttv[\"description\"] = [\"LSMDC Test 1000 x (Test Set[mean frame], Corresponding sentences)\"]\n",
        "LSMDC_metrics_ttv[\"task\"] = [\"Text-to-video\"]\n",
        "LSMDC_metrics_ttv[\"dataset\"] = [\"LSMDC\"]\n",
        "LSMDC_metrics_ttv = pd.DataFrame(LSMDC_metrics_ttv)\n",
        "print(LSMDC_metrics_ttv)\n",
        "LSMDC_metrics_ttv.to_csv(\"test.csv\", mode='a', header=False)\"\"\""
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LSMDC_test_video_dict = torch.load(LSMDC_path + \"LSMDC_test_CLIP_visual_1_2.pt\",map_location=torch.device(\\'cpu\\'))\\nLSMDC_test_text_dict = torch.load(LSMDC_path + \"LSMDC_test_dict_CLIP_text.pt\",map_location=torch.device(\\'cpu\\'))\\nLSMDC_text_matrix, LSMDC_aux_text = stack_encoded_dict(LSMDC_test_text_dict, LSMDC_test_text_dict.keys())\\nLSMDC_video_matrix, LSMDC_aux_video = stack_encoded_dict(LSMDC_test_video_dict, LSMDC_test_text_dict.keys(), lambda x : normalize_matrix(torch.mean(x, dim = 0, keepdim = True)))\\nLSMDC_text_matrix.shape, LSMDC_video_matrix.shape\\nLSMDC_metrics_vtt = rank_at_k_precomputed(LSMDC_video_matrix @ LSMDC_text_matrix.T)\\n\\nLSMDC_metrics_vtt[\"description\"] = [\"LSMDC Test 1000 x (Test Set[mean frame], Corresponding sentences)\"]\\nLSMDC_metrics_vtt[\"task\"] = [\"Video-to-text\"]\\nLSMDC_metrics_vtt[\"dataset\"] = [\"LSMDC\"]\\nLSMDC_metrics_vtt = pd.DataFrame(LSMDC_metrics_vtt)\\nprint(LSMDC_metrics_vtt)\\nLSMDC_metrics_vtt.to_csv(\"test.csv\", mode=\\'a\\', header=False)\\nLSMDC_metrics_ttv, LSMDC_diagonal_ttv = rank_at_k_precomputed(LSMDC_text_matrix @ LSMDC_video_matrix.T, diag = True)\\n\\nLSMDC_metrics_ttv[\"description\"] = [\"LSMDC Test 1000 x (Test Set[mean frame], Corresponding sentences)\"]\\nLSMDC_metrics_ttv[\"task\"] = [\"Text-to-video\"]\\nLSMDC_metrics_ttv[\"dataset\"] = [\"LSMDC\"]\\nLSMDC_metrics_ttv = pd.DataFrame(LSMDC_metrics_ttv)\\nprint(LSMDC_metrics_ttv)\\nLSMDC_metrics_ttv.to_csv(\"test.csv\", mode=\\'a\\', header=False)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}